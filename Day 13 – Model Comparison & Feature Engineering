# Day 13 â€“ Model Comparison & Feature Engineering  
**Databricks 14-Days AI Challenge**

## ğŸ“Œ Topic
Model Comparison and Feature Engineering

---

## ğŸ¯ Challenge Objectives
- Train **3 different machine learning models**
- Compare evaluation metrics using **MLflow**
- Build a **Spark-based ML workflow**
- Select the best-performing model based on metrics

---

## ğŸ“Š Dataset
Source: Spark table `Copy_of_movies`

### Selected Features
- `Release_year`
- `Budget`
- `Revenue`

### Target
- `Imdb_rating`

Only numeric columns were used, and null values were removed before training.

---

## âš™ï¸ Models Trained
- Linear Regression  
- Decision Tree Regressor  
- Random Forest Regressor  

---

## ğŸ§ª Experiment Tracking
All experiments were tracked using **MLflow**, including:
- Model parameters  
- RÂ² evaluation metric  
- Trained model artifacts  

Each model run was logged under a single MLflow experiment for comparison.

---

## ğŸ“ˆ Results (RÂ² Score)

| Model            | RÂ² Score |
|------------------|----------|
| Linear Regression | 0.0527 |
| Decision Tree     | **0.2076** |
| Random Forest     | 0.1298 |

âœ… **Decision Tree Regressor** performed better compared to other models for this dataset.

---

## ğŸ’¡ Key Learnings
- Model performance depends heavily on **feature quality**, not just algorithm choice  
- MLflow simplifies **experiment tracking and model comparison**  
- Low RÂ² scores highlight the need for **better feature engineering and data enrichment**  
- Transparent evaluation helps guide iterative model improvement  

---

## ğŸ› ï¸ Tech Stack
- Apache Spark (Databricks)
- Python
- scikit-learn
- MLflow

---

## âœï¸ Author
**Sankeerthi Reddi**  
Aspiring Data Analyst | Data Science Enthusiast  
ğŸ”— LinkedIn: https://www.linkedin.com/posts/maddinareddisankeerthi_databrickswithidc-codebasics-indiandataclub-activity-7419796395282493440-7nVJ?utm_source=share&utm_medium=member_desktop&rcm=ACoAAFxGtfwBB5nhO5M2F5A9lGeNvQx8T-786JY
